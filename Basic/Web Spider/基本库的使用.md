一、urllib

urllib是Python内置的HTTP请求库，不需要额外安装即可使用。
urllib库包含了4个模块：

    1.1、request：最基本的HTTP请求模块，可以用来模拟发送请求。
                  只需要给库方法传入URL以及额外的参数，就可以模拟浏览器访问网址。
   
    1.2、error：异常处理模块，如果出现请求错误，可以捕获这些异常，然后进行重试或其他操作以保证程序不会意外终止。
   
    1.3、parse：工具模块，提供了许多URL处理方法，比如拆分、解析、合并等。
   
    1.4、robotparser：主要是用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬，用得比较少。
   
1、发送请求

使用urllib的request模块，可以方便地实现请求地发送并得到响应。

1.1、urlopen()方法
是urllib.request()模块提供的最基本的构造HTTP请求的方法，利用它可以模拟浏览器的一个请求发起过程。同时还带有处理授权验证(authenticaton)、重定向(redirection)、浏览器Cookies以及其他内容。利用最基本的urlopen()方法，可以完成最基本的简单网页的GET请求抓取。

urlopen()方法抓取网页源码(包括链接、图片地址、文本信息)后，可以利用read()方法得到返回的网页内容。
    
urlopen()的API用法：  

urllib.request.urlopen(url, data=None, [timeout]*, cafile=None, capath=None, cadefault=False, context=None)  
    
    1.2.1、data参数
           可选择是否添加该参数。若添加，需要使用bytes()方法将参数转化为字节流编码格式的内容，即bytes类型。
           另外，如果传递了该参数，则请求方式应从GET方式给位POST方式。
    1.2.2、timeout参数
           用于设置超时时间，单位为秒。如果请求超过了设置的这个时间，还没得到响应，就会抛出异常。
           如果不指定该参数，就会使用全局默认时间。
           支持HTTP、HTTPS、FTP请求。
               
    1.2.3、context参数
           必须是ssl.SSLConText类型，用来指定SSL设置。
    1.2.4、cafile参数和capath参数
           分别用来指定CA证书和它的路径，在请求HTTPS链接时会有用。
    2.2.5、cadefailt参数
           默认值为False，现已弃用。

1.3、Request

利用urlopen()方法只能实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求。如果请求中需加入Headers等信息，就要利用更强大的Request类来构建。

利用Request()方法，一方面可以将请求独立成一个对象，另一方面可以更加丰富和灵活地配置参数。
    
    例：
    request = urllib.request.Request(URL)
    
    reponse = urllib.request.urlopen(request)
Request的构造方法:  

    class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)

    (1)、参数url是必传参数，用于请求URL，其他都为选传。
    
    (2)、参数data，如果要传，必须传bytes(字节流)类型的。如果它是字典，可以先用urllib.parse模块中的urlencode()编码
    
    (3)、参数headers是请求头，是一个字典。可以在构建请求时通过headers参数直接构建。
         添加请求头最常见的方法时通过修改User-Agent来伪装浏览器，默认的User-Agent是Python—urllib。
         
    (4)、参数origin_req_host指的是请求方的host名称或者IP地址。
    
    (5)、参数unverifiable表示这个请求是否是无法验证的，默认是False，意思是用户没有足够的权限来选择接受这个请求的结果。
    
    (6)、参数method是一个字符串，用来指示请求使用的方法，比如POST、GET、PUT等。

1.4、Handler

用于一些更加高级的操作，比如Cookies处理，代理设置等。可以将其理解为各种处理器，有专门处理登陆验证的，有处理Cookies的，有处理代理设置的。利用它们，几乎可以做到HTTP请求中的所有事情。

1.4.1、BaseHandler类，是所有其他Handler的父类，提供了最基本的方法，例如default_open()、protocol_request()等。
    
     BaseHandler的子类：
     (1)、HTTPDefaultErrorHandler：用于处理HTTP响应错误，错误都会抛出HTTPError类型的异常。
     
     (2)、HTTPRedirectHandler：用于处理重定向。
     
     (3)、HTTPCookieProcessor：用于处理Cookies。
     
     (4)、ProxyHandler：用于设置代理，默认代理为空。
     
     (5)、HTTPPasswordMgr：用于管理密码，它维护了用户名和密码的表。
     
     (6)、HTTPBasicAuthHandler：用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。
     
     ......
     
1.5、OpenerDirector类

Request和urlop()封装好的极其常用的请求方法，利用它们可以完成基本的请求。为了实现更高级的功能，需要深入一层地进行配置，使用更底层的实例来完成操作，因此引入OpenerDirector类。OpenerDirector可以称为Opener。   
Opener可以使用open()方法，返回的类型与urlopen()一致。Opener与Handler的关系，简而言之，就是利用Handler来构建Opener。

......

2、处理异常

urllib的error模块定义了由request模块产生的异常。如果出现问题，request模块便会抛出error模块中定义的异常。

2.1、URLError

URLError类来自urllib库的error模块，它继承自OSError类，是error类异常处理模块的基类，由request模块产生的异常都可以通过捕获这个类来处理。
它具有一个属性reason，即返回错误的原因。

    例：
    from urllib import request, error
    
    try:
        response = request.urlopen(URL)
    except error.URLError as e:
        print(e.reason)
若URL不存在，抛出异常，会输出结果：
    
       Not Found
通过这样的操作，可以避免程序异常终止，同时异常得到了有效处理。

2.2、HTTPError

是URLError的子类，专门用来处理HTTP请求错误，比如认证请求失败等。有如下三个属性。

    (1)、code：返回HTTP状态码，比如404表示网页不存在，500表示服务器内部错误等。
    
    (2)、reason：同父类一样，用于返回错误的原因。
    
    (3)、headers：返回请求头。
    
3、解析链接

urllib库提供的parse模块，定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换。它支持如下协议的URL处理：file、ftp、gopther、hdl、http、https、imap、mailto、mms、news、nntp、prospero、rsync、rtsp、sftp、sip、sips、snews、svn、svn+ssh、telnet和wais。

3.1、urlparse()

该方法可以实现URL的识别和分段。

    例：
    from urllib.parse import urlparse
    
    result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')
    print(type(result), result)
这里利用urlparse()方法进行了一个URL的解析，首先，输出了解析结果的类型，然后将结果也输出。

    结果：
    <class 'urllib.parse.ParseResult'>
    ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', 
    query='id=5', fragment='comment')
返回结果是一个ParseResult类型的对象，它包含6部分，scheme、netloc、path、params、query和fragment。

观察URL：
    
    http://www.baidu.com/index.html;user?id=5#comment
可以发现，urlparse()方法将其拆分成了6部分。大体观察可以发现，解析时有特定的分隔符。比如，://前面的就是scheme，代表协议；第一个/前面便是netloc，即域名；分号;前面是params，代表参数。

可以得出一个标准的链接格式：

    scheme://netloc/path;parameters?query#fragment
    
一个标准的URL都会符合这个规则，利用urlparse()方法可以将它拆分开来。

urlparse()方法的API用法：

urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)

    (1)、urlstring：必填项，即待解析的URL.
    
    (2)、scheme：默认协议(如http或https等)。如果这个链接没有带协议信息，会将默认这个作为默认协议(https)。
    
    (3)、allow_fragments：即是否忽略fragment。如果它被设置为False，fragment部分就会被忽略，
         它会被解析为path、parameters或者query的一部分，而fragment部分为空。
         
3.2、urlunparse()

urlunparse()是urlparse()的对立方法。它接受的参数是一个可迭代对象，但是它的长度必须是6，否则会抛出参数数量不足或者过多的问题。

    例：
    from urllib.parse import urlunparse
    
    data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
    print(urlunparse(data))
    
    结果如下：	
    http://www.baidu.com/index.html;user?a=6#comment
这样就可以成功实现URL的构造。

3.3、urlsplit()

和urlparse()方法非常相似，只不过它不再单独解析params这一部分，只返回5个结果。上面例子中的params会合并到path中。

    例：
    from urllib.parse import urlsplit
    
    result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')
    print(result)
    
    结果如下：
    SplitResult(scheme='http', netloc='www.baidu.com', path='/index.html;user', query='id=5', fragment='comment')
返回结果是SplitResult，它其实是一个元组类型，既可以用属性获取值，也可以用索引来获取。 

3.4、urlunsplit()

与urlunparse()类似，它也是将链接各个部分组合成完整链接的方法，传入的参数也是一个可迭代对象，例如列表、元组等，唯一的区别是长度必须为5。

    例：
    from urllib.parse import urlunsplit
    
    data = ['http', 'www.baidu.com', 'index.html', 'a=6', 'comment']
    print(urlunsplit(data))
    
    结果：
    http://www.baidu.com/index.html?a=6#comment
    
3.5、urljoin()

有了urlunparse()和urlunsplit()方法，我们可以完成链接的合并，不过前提必须要有特定长度的对象，链接的每一部分都要清晰分开。此外，生成链接还有另一个方法，那就是urljoin()方法。我们可以提供一个base_url（基础链接）作为第一个参数，将新的链接作为第二个参数，该方法会分析base_url、scheme、netloc和path这3个内容并对新链接缺失的部分进行补充，最后返回结果。

    例：
    from urllib.parse import urljoin
 
    print(urljoin('http://www.baidu.com', 'FAQ.html'))
    print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))
    print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))
    print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))
    print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))
    print(urljoin('http://www.baidu.com', '?category=2#comment'))
    print(urljoin('www.baidu.com', '?category=2#comment'))
    print(urljoin('www.baidu.com#comment', '?category=2'))
    
    结果：
    http://www.baidu.com/FAQ.html
    https://cuiqingcai.com/FAQ.html
    https://cuiqingcai.com/FAQ.html
    https://cuiqingcai.com/FAQ.html?question=2
    https://cuiqingcai.com/index.php
    http://www.baidu.com?category=2#comment
    www.baidu.com?category=2#comment
    www.baidu.com?category=2
可以发现，base_url提供了三项内容scheme、netloc和path。如果这3项在新的链接里不存在，就予以补充；如果新的链接存在，就使用新的链接的部分。而base_url中的params、query和fragment是不起作用的。
通过urljoin()方法，我们可以轻松实现链接的解析、拼合与生成。

3.6、urlencode()

在构造GET请求参数的时候非常有用。

    例：
    from urllib.parse import urlencode
    
    params = {
        'name': 'germey',
        'age': 22
    }
    base_url = 'http://www.baidu.com?'
    url = base_url + urlencode(params)
    print(url)
这里首先声明了一个字典来将参数表示出来，然后调用urlencode()方法将其序列化为GET请求参数。

    结果如下：
    http://www.baidu.com?name=germey&age=22
参数成功地由字典类型转化为GET请求参数。这个方法非常常用。有时为了更加方便地构造参数，我们会事先用字典来表示。要转化为URL的参数时，只需要调用该方法即可。

3.7、parse_qs()

有了序列化，必然就有反序列化。如果我们有一串GET请求参数，利用parse_qs()方法，就可以将它转回字典。

    例：
    from urllib.parse import parse_qs
    
    query = 'name=germey&age=22'
    print(parse_qs(query))
    
    结果：
    {'name': ['germey'], 'age': ['22']}
成功转回为字典类型了。

3.8、parse_qsl()

用于将参数转化为元组组成的列表。

    例：
    from urllib.parse import parse_qsl
    
    query = 'name=germey&age=22'
    print(parse_qsl(query))
    
    结果：
    [('name', 'germey'), ('age', '22')]
运行结果是一个列表，而列表中的每一个元素都是一个元组，元组的第一个内容是参数名，第二个内容是参数值。

3.9、quote()

该方法可以将内容转化为URL编码的格式。URL中带有中文参数时，有时可能会导致乱码的问题，此时用这个方法可以将中文字符转化为URL编码。

    例：
    from urllib.parse import quote
    
    keyword = '壁纸'
    url = 'https://www.baidu.com/s?wd=' + quote(keyword)
    print(url)
这里声明了一个中文的搜索文字，然后用quote()方法对其进行URL编码。   
    
    结果：
    https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8
    
3.10、unquote()
可以进行URL解码。

    例：
    from urllib.parse import unquote
    
    url = 'https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'
    print(unquote(url))
    
    结果：
    https://www.baidu.com/s?wd=壁纸
利用unquote()方法可以方便地实现解码

4、分析Robots协议  

利用urllib的robotparser模块，我们可以实现网站Robots协议的分析。

4.1、Robots协议

Robots协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作robots.txt的文本文件，一般放在网站的根目录下。当搜索爬虫访问一个站点时，它首先会检查这个站点根目录下是否存在robots.txt文件，如果存在，搜索爬虫会根据其中定义的爬取范围来爬取。如果没有找到这个文件，搜索爬虫便会访问所有可直接访问的页面。

    样例：
    User-agent: *
    Disallow: /
    Allow: /public/
这实现了对所有搜索爬虫只允许爬取public目录的功能，将上述内容保存成robots.txt文件，放在网站的根目录下，和网站的入口文件（比如index.php、index.html和index.jsp等）放在一起。
上面的User-agent描述了搜索爬虫的名称，这里将其设置为*则代表该协议对任何爬取爬虫有效。

    例：
    User-agent: Baiduspider
这就代表我们设置的规则对百度爬虫是有效的。如果有多条User-agent记录，则就会有多个爬虫会受到爬取限制，但至少需要指定一条。  
Disallow指定了不允许抓取的目录，比如上例子中设置为/则代表不允许抓取所有页面。  
Allow一般和Disallow一起使用，一般不会单独使用，用来排除某些限制。现在我们设置为/public/，则表示所有页面不允许抓取，但可以抓取public目录。

4.2、爬虫名称
爬虫是有固定的名称的，一些常见的搜索爬虫的名称及对应的网站:

   ![image](https://github.com/Lee295/Python-and-Web-spider/blob/master/Basic/Web%20Spider/images/P1.png)
    
4.3、robotparser

robotparser模块是用来解析robots.txt的。该模块提供了一个类RobotFileParser，它可以根据某网站的robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。  
该类的使用，只需要在构造方法里传入robots.txt的链接即可。

    声明：
    urllib.robotparser.RobotFileParser(url='')
可以在声明时不传入，默认为空，最后再使用set_url()方法设置一下即可。

常用方法：

    (1)、set_url()：用来设置robots.txt文件的链接。如果在创建RobotFileParser对象时传入了链接，
         那么就不需要再使用这个方法设置了。
    
    (2)、read()：读取robots.txt文件并进行分析。注意————这个方法执行一个读取和分析操作，如果不调用这个方法，
         接下来的判断都会为False，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。
    
    (3)、parse()：用来解析robots.txt文件，传入的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。
    
    (4)、can_fetch()：该方法传入两个参数，第一个是User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否
         可以抓取这个URL，返回结果是True或False。
    
    (5)、mtime()：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，
         可能需要定期检查来抓取最新的robots.txt。
    
    (6)、modified()：它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析robots.txt的时间。
    
    例：
    from urllib.robotparser import RobotFileParser
    
    rp = RobotFileParser()
    rp.set_url('http://www.jianshu.com/robots.txt')
    rp.read()
    print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))
    print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&page=1&type=collections"))
这里以简书为例，首先创建RobotFileParser对象，然后通过set_url()方法设置了robots.txt的链接。当然，不用这个方法的话，可以在声明时直接用如下方法设置：
    
    rp = RobotFileParser('http://www.jianshu.com/robots.txt')
接着利用can_fetch()方法判断了网页是否可以被抓取。
    
    结果：
    True
    False
    
同样可以使用parse()方法执行读取和分析。

    例：
    from urllib.robotparser import RobotFileParser
    from urllib.request import urlopen
    
    rp = RobotFileParser()
    rp.parse(urlopen('http://www.jianshu.com/robots.txt').read().decode('utf-8').split('\n'))
    print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))
    print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&page=1&type=collections"))
运行结果一样：

    True
    False
    
    
二、requests的使用

1、基本用法

urllib库中的urlopen()方法实际上是以GET方式请求网页，而requests中相应的方法就是get()方法

    例：
    import requests
 
    r = requests.get('https://www.baidu.com/')
    print(type(r))
    print(r.status_code)
    print(type(r.text))
    print(r.text)
    print(r.cookies)
    
    结果：
    <class 'requests.models.Response'>
    200
    <class 'str'>
    <html>
    <head>
        <script>
            location.replace(location.href.replace("https://","http://"));
        </script>
    </head>
    <body>
        <noscript><meta http-equiv="refresh" content="0;url=http://www.baidu.com/"></noscript>
    </body>
    </html>
    <RequestsCookieJar[<Cookie BIDUPSID=992C3B26F4C4D09505C5E959D5FBC005 for .baidu.com/>, 
    <Cookie PSTM=1472227535 for .baidu.com/>, 
    <Cookie __bsi=15304754498609545148_00_40_N_N_2_0303_C02F_N_N_N_0 for .www.baidu.com/>, 
    <Cookie BD_NOT_HTTPS=1 for www.baidu.com/>]>
这里调用get()方法实现与urlopen()相同的操作，得到一个Response对象，然后分别输出了Response的类型、状态码、响应体的类型、内容以及Cookies。  
通过运行结果可以发现，它的返回类型是requests.models.Response，响应体的类型是字符串str，Cookies的类型是RequestsCookieJar。

使用get()方法成功实现一个GET请求，这倒不算什么，更方便之处在于其他的请求类型依然可以用一句话来完成

    例:r = requests.post('http://httpbin.org/post')
       r = requests.put('http://httpbin.org/put')
       r = requests.delete('http://httpbin.org/delete')
       r = requests.head('http://httpbin.org/get')
       r = requests.options('http://httpbin.org/get')
这里分别用post()、put()、delete()等方法实现了POST、PUT、DELETE等请求。

1.1、GET请求

GET请求是HTTP中最常见的请求之一。
