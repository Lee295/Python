一、urllib

urllib是Python内置的HTTP请求库，不需要额外安装即可使用。
urllib库包含了4个模块：

    1.1、request：最基本的HTTP请求模块，可以用来模拟发送请求。
                  只需要给库方法传入URL以及额外的参数，就可以模拟浏览器访问网址。
   
    1.2、error：异常处理模块，如果出现请求错误，可以捕获这些异常，然后进行重试或其他操作以保证程序不会意外终止。
   
    1.3、parse：工具模块，提供了许多URL处理方法，比如拆分、解析、合并等。
   
    1.4、robotparser：主要是用来识别网站的robots.txt文件，然后判断哪些网站可以爬，哪些网站不可以爬，用得比较少。
   
1、发送请求

使用urllib的request模块，可以方便地实现请求地发送并得到响应。

1.1、urlopen()方法
是urllib.request()模块提供的最基本的构造HTTP请求的方法，利用它可以模拟浏览器的一个请求发起过程。同时还带有处理授权验证(authenticaton)、重定向(redirection)、浏览器Cookies以及其他内容。利用最基本的urlopen()方法，可以完成最基本的简单网页的GET请求抓取。

urlopen()方法抓取网页源码(包括链接、图片地址、文本信息)后，可以利用read()方法得到返回的网页内容。
    
urlopen()的API用法：  

urllib.request.urlopen(url, data=None, [timeout]*, cafile=None, capath=None, cadefault=False, context=None)  
    
    1.2.1、data参数
           可选择是否添加该参数。若添加，需要使用bytes()方法将参数转化为字节流编码格式的内容，即bytes类型。
           另外，如果传递了该参数，则请求方式应从GET方式给位POST方式。
    1.2.2、timeout参数
           用于设置超时时间，单位为秒。如果请求超过了设置的这个时间，还没得到响应，就会抛出异常。
           如果不指定该参数，就会使用全局默认时间。
           支持HTTP、HTTPS、FTP请求。
               
    1.2.3、context参数
           必须是ssl.SSLConText类型，用来指定SSL设置。
    1.2.4、cafile参数和capath参数
           分别用来指定CA证书和它的路径，在请求HTTPS链接时会有用。
    2.2.5、cadefailt参数
           默认值为False，现已弃用。

1.3、Request

利用urlopen()方法只能实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求。如果请求中需加入Headers等信息，就要利用更强大的Request类来构建。

利用Request()方法，一方面可以将请求独立成一个对象，另一方面可以更加丰富和灵活地配置参数。
    
    例：
    request = urllib.request.Request(URL)
    
    reponse = urllib.request.urlopen(request)
Request的构造方法:  

    class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)

    (1)、参数url是必传参数，用于请求URL，其他都为选传。
    
    (2)、参数data，如果要传，必须传bytes(字节流)类型的。如果它是字典，可以先用urllib.parse模块中的urlencode()编码
    
    (3)、参数headers是请求头，是一个字典。可以在构建请求时通过headers参数直接构建。
         添加请求头最常见的方法时通过修改User-Agent来伪装浏览器，默认的User-Agent是Python—urllib。
         
    (4)、参数origin_req_host指的是请求方的host名称或者IP地址。
    
    (5)、参数unverifiable表示这个请求是否是无法验证的，默认是False，意思是用户没有足够的权限来选择接受这个请求的结果。
    
    (6)、参数method是一个字符串，用来指示请求使用的方法，比如POST、GET、PUT等。

1.4、Handler

用于一些更加高级的操作，比如Cookies处理，代理设置等。可以将其理解为各种处理器，有专门处理登陆验证的，有处理Cookies的，有处理代理设置的。利用它们，几乎可以做到HTTP请求中的所有事情。

1.4.1、BaseHandler类，是所有其他Handler的父类，提供了最基本的方法，例如default_open()、protocol_request()等。
    
     BaseHandler的子类：
     (1)、HTTPDefaultErrorHandler：用于处理HTTP响应错误，错误都会抛出HTTPError类型的异常。
     
     (2)、HTTPRedirectHandler：用于处理重定向。
     
     (3)、HTTPCookieProcessor：用于处理Cookies。
     
     (4)、ProxyHandler：用于设置代理，默认代理为空。
     
     (5)、HTTPPasswordMgr：用于管理密码，它维护了用户名和密码的表。
     
     (6)、HTTPBasicAuthHandler：用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。
     
     ......
     
1.5、OpenerDirector类

Request和urlop()封装好的极其常用的请求方法，利用它们可以完成基本的请求。为了实现更高级的功能，需要深入一层地进行配置，使用更底层的实例来完成操作，因此引入OpenerDirector类。OpenerDirector可以称为Opener。   
Opener可以使用open()方法，返回的类型与urlopen()一致。Opener与Handler的关系，简而言之，就是利用Handler来构建Opener。

......

2、处理异常

urllib的error模块定义了由request模块产生的异常。如果出现问题，request模块便会抛出error模块中定义的异常。

2.1、URLError

URLError类来自urllib库的error模块，它继承自OSError类，是error类异常处理模块的基类，由request模块产生的异常都可以通过捕获这个类来处理。
它具有一个属性reason，即返回错误的原因。

    例：
    from urllib import request, error
    try:
        response = request.urlopen(URL)
    except error.URLError as e:
        print(e.reason)
若URL不存在，抛出异常，会输出结果：
    
       Not Found
通过这样的操作，可以避免程序异常终止，同时异常得到了有效处理。

2.2、HTTPError

是URLError的子类，专门用来处理HTTP请求错误，比如认证请求失败等。有如下三个属性。

    (1)、code：返回HTTP状态码，比如404表示网页不存在，500表示服务器内部错误等。
    
    (2)、reason：同父类一样，用于返回错误的原因。
    
    (3)、headers：返回请求头。
    
3、解析链接

urllib库提供的parse模块，定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换。它支持如下协议的URL处理：file、ftp、gopther、hdl、http、https、imap、mailto、mms、news、nntp、prospero、rsync、rtsp、sftp、sip、sips、snews、svn、svn+ssh、telnet和wais。

3.1、urlparse()

该方法可以实现URL的识别和分段。

    例：
    from urllib.parse import urlparse
    result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')
    print(type(result), result)
这里利用urlparse()方法进行了一个URL的解析，首先，输出了解析结果的类型，然后将结果也输出。

    结果：
    <class 'urllib.parse.ParseResult'>
    ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment')
返回结果是一个ParseResult类型的对象，它包含6部分，scheme、netloc、path、params、query和fragment。

观察URL：
    
    http://www.baidu.com/index.html;user?id=5#comment
可以发现，urlparse()方法将其拆分成了6部分。大体观察可以发现，解析时有特定的分隔符。比如，://前面的就是scheme，代表协议；第一个/前面便是netloc，即域名；分号;前面是params，代表参数。

可以得出一个标准的链接格式：

    scheme://netloc/path;parameters?query#fragment
    
一个标准的URL都会符合这个规则，利用urlparse()方法可以将它拆分开来。

urlparse()方法的API用法：

urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)

    (1)、urlstring：必填项，即待解析的URL.
    
    (2)、scheme：默认协议(如http或https等)。如果这个链接没有带协议信息，会将默认这个作为默认协议(https)。
    
    (3)、allow_fragments：即是否忽略fragment。如果它被设置为False，fragment部分就会被忽略，
         它会被解析为path、parameters或者query的一部分，而fragment部分为空。
         
3.2、urlunparse()

urlunparse()是urlparse()的对立方法。它接受的参数是一个可迭代对象，但是它的长度必须是6，否则会抛出参数数量不足或者过多的问题。

    例：
    from urllib.parse import urlunparse
    data = ['http', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
    print(urlunparse(data))
    
    结果如下：	
    http://www.baidu.com/index.html;user?a=6#comment
这样就可以成功实现URL的构造。

3.3、urlsplit()

和urlparse()方法非常相似，只不过它不再单独解析params这一部分，只返回5个结果。上面例子中的params会合并到path中。

    例：
    from urllib.parse import urlsplit
    result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')
    print(result)
    
    结果如下：
    SplitResult(scheme='http', netloc='www.baidu.com', path='/index.html;user', query='id=5', fragment='comment')
返回结果是SplitResult，它其实是一个元组类型，既可以用属性获取值，也可以用索引来获取。 

3.4、urlunsplit()

与urlunparse()类似，它也是将链接各个部分组合成完整链接的方法，传入的参数也是一个可迭代对象，例如列表、元组等，唯一的区别是长度必须为5。

    例：
    from urllib.parse import urlunsplit
    data = ['http', 'www.baidu.com', 'index.html', 'a=6', 'comment']
    print(urlunsplit(data))
    
    结果：
    http://www.baidu.com/index.html?a=6#comment
    
3.5、urljoin()

有了urlunparse()和urlunsplit()方法，我们可以完成链接的合并，不过前提必须要有特定长度的对象，链接的每一部分都要清晰分开。此外，生成链接还有另一个方法，那就是urljoin()方法。我们可以提供一个base_url（基础链接）作为第一个参数，将新的链接作为第二个参数，该方法会分析base_url、scheme、netloc和path这3个内容并对新链接缺失的部分进行补充，最后返回结果。

    例：
    from urllib.parse import urljoin
 
    print(urljoin('http://www.baidu.com', 'FAQ.html'))
    print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))
    print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))
    print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))
    print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))
    print(urljoin('http://www.baidu.com', '?category=2#comment'))
    print(urljoin('www.baidu.com', '?category=2#comment'))
    print(urljoin('www.baidu.com#comment', '?category=2'))
    
    结果：
    http://www.baidu.com/FAQ.html
    https://cuiqingcai.com/FAQ.html
    https://cuiqingcai.com/FAQ.html
    https://cuiqingcai.com/FAQ.html?question=2
    https://cuiqingcai.com/index.php
    http://www.baidu.com?category=2#comment
    www.baidu.com?category=2#comment
    www.baidu.com?category=2
可以发现，base_url提供了三项内容scheme、netloc和path。如果这3项在新的链接里不存在，就予以补充；如果新的链接存在，就使用新的链接的部分。而base_url中的params、query和fragment是不起作用的。
通过urljoin()方法，我们可以轻松实现链接的解析、拼合与生成。

3.6、urlencode()

在构造GET请求参数的时候非常有用。

    例：
    from urllib.parse import urlencode
    params = {
        'name': 'germey',
        'age': 22
    }
    base_url = 'http://www.baidu.com?'
    url = base_url + urlencode(params)
    print(url)
这里首先声明了一个字典来将参数表示出来，然后调用urlencode()方法将其序列化为GET请求参数。

    结果如下：
    http://www.baidu.com?name=germey&age=22
参数成功地由字典类型转化为GET请求参数。这个方法非常常用。有时为了更加方便地构造参数，我们会事先用字典来表示。要转化为URL的参数时，只需要调用该方法即可。

3.7、parse_qs()

有了序列化，必然就有反序列化。如果我们有一串GET请求参数，利用parse_qs()方法，就可以将它转回字典。

    例：
    from urllib.parse import parse_qs
    query = 'name=germey&age=22'
    print(parse_qs(query))
    
    结果：
    {'name': ['germey'], 'age': ['22']}
成功转回为字典类型了。

3.8、parse_qsl()

用于将参数转化为元组组成的列表。

    例：
    from urllib.parse import parse_qsl
    query = 'name=germey&age=22'
    print(parse_qsl(query))
    
    结果：
    [('name', 'germey'), ('age', '22')]
运行结果是一个列表，而列表中的每一个元素都是一个元组，元组的第一个内容是参数名，第二个内容是参数值。

3.9、quote()

该方法可以将内容转化为URL编码的格式。URL中带有中文参数时，有时可能会导致乱码的问题，此时用这个方法可以将中文字符转化为URL编码。

    例：
    from urllib.parse import quote
    keyword = '壁纸'
    url = 'https://www.baidu.com/s?wd=' + quote(keyword)
    print(url)
这里声明了一个中文的搜索文字，然后用quote()方法对其进行URL编码。   
    
    结果：
    https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8
    
3.10、unquote()
可以进行URL解码。

    例：
    from urllib.parse import unquote
    url = 'https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'
    print(unquote(url))
    
    结果：
    https://www.baidu.com/s?wd=壁纸
利用unquote()方法可以方便地实现解码
